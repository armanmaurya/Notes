Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitude or values or units. If **feature scaling** is not done, then a [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.
## Why use Feature Scaling?
In [Machine Learning](../../Machine%20Learning/Machine%20Learning.md), feature scaling is employed for a number of purposes:
- Scaling guarantees that all features are on a comparable scale and have comparable ranges. This process is know as feature normalisation. This is significant because the magnitude off the features has an impact on many [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) techniques. Larger scale features may dominate the learning process and have an excessive impact on the outcomes. You can avoid this problem and make sure that each feature contributes equally to the learning process by scaling the features.
- **Algorithm performance improvement:** When the features are scaled, several [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) methods, including gradient descent-based algorithms, distance-based algorithms (such k-nearest neighbors), and support vector machines perform better on converge more quickly. The algorithm's performance can be enhanced by scaling the features, which can hasten the convergence of the algorithm to the ideal outcome.
- **Preventing numerical instability**: Numerical instability can be prevented by avoiding significant scale disparities between features. Examples include distance calculations or matrix operations, where having features with radically differing scales can result in numerical overflow or underflow problems. Stable computations are ensured and these issues are mitigated by scaling the features.
- Scaling features makes ensuring that each characteristic is given the same consideration during the learning process. Without scaling, bigger scale features could dominate the learning, producing skewed outcomes. This bias is removed through scaling, which also guarantees that each feature contributes fairly to model predictions.