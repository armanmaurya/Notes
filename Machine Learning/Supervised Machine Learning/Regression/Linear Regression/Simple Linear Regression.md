This is the simplest form of linear Regression, and it involves only one independent variable and one dependent variable. This equation for simple linear regression is:
$$
y = mx +c
$$
Where, 
- $y$ is the dependent variable
- $x$ is the independent variable 
- $m$ is the slope
- $c$ is the intercept
## Loss Function
Simple Linear Regression commonly use [Mean Squared Error](Mean%20Squared%20Error.md).
$$\text{Mean Squared Error} = \frac{1}{n}\sum_{i = 1}^{n}(Y_i – \hat Y_i)^2$$
Where:
- $n$ is the number of observations in the dataset.
- $Y_{i}$ is the actual value of the $i-th$ data points.
- $\hat Y_i = mX_{i}+c$ : is the predicted value of the $i^{th}$ observation.
## Optimization
To Optimize the **Simple Linear Regression** model we have to minimize the error generated by Mean Squared Error, to minimize this error, we have to adjust the model parameters which in this case is **slope (m)** and **intercept (c)**, by adjusting means we have to find the near correct value of **slope (m)** and **intercept (c)**.

To find the value of m and c, we partially differentiate the [Mean Squared Error](Mean%20Squared%20Error.md) with respect to $m$ and $c$
1. Partially Differentiate $w.r.to$ $m$ 